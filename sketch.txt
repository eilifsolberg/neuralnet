Sparse implementation?
Basic functionality that will not change, implement in classes,
other functionality implement in separate function.
Make sure you can get all the info from the classes, don't have to
change them later.

need some plotting of the loss function
what about regularization? dropout etc? early stopping?

Class NeuralNet
      Layers
      --loss_function

      __init__(list of layers, --loss_function)

      def forward_propagate(x)
      def forward_propagate_store(x)
      def back_propagate(x,y)

      def fit(iterations) also calculate loss somehow?
      	  what about step-size?
	       adagrad? constant?
	       have a generator?
	       
Class Layer
      weightmatrix: number_of_output x number of input (sparse implementation?)
      bias: number_of_output
      activation_function
      

class activation_function

      def value(x)
      def deriv(x)

class lossfunction

      def loss(y_hat, y)
      def deriv(y_hat, y)
